nameOverride: ""
fullnameOverride: "llm-serving"
podAnnotations: {}

service:
  image:
    version: "vllm/vllm-openai:v0.7.0"
    pullPolicy: IfNotPresent
    custom:
      enabled: false
      version: ""

resources:
  limits:
    gpu:
      number: 1

huggingFace: 
  hfToken: ""

llm:
  hfModelName: meta-llama/Llama-3.2-1B-Instruct
  localPath: "/root/.cache/huggingface"
  memoryUtilization: 0.8
  dtype: "half"
  maxModelLen": 8208

networking:
  port:
    number: 8000

ingress:
    enabled: true
    className: ""
    annotations: 
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    hostname: "chart-example.local"

s3:
  enabled: true # Set to true to use S3
  s3ModelPath: ""
  accessKeyId: ""
  endpoint: ""
  defaultRegion: ""
  secretAccessKey: ""
  sessionToken: ""

nodeSelector: {}