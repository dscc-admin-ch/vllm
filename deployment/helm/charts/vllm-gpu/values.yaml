global:
  suspend: false # Allow scaling the app to 0

nameOverride: ""
fullnameOverride: "llm-serving"
podAnnotations: {}

service:
  type: ClusterIP
  nodePort:
  image:
    version: "vllm/vllm-openai:v0.8.2"
    pullPolicy: IfNotPresent
    custom:
      enabled: false
      version: ""

resources:
  limits:
    gpu:
      number: 1

huggingFace:
  hfToken: ""

llm:
  hfModelName: meta-llama/Llama-3.2-1B-Instruct
  localPath: "/root/.cache/huggingface"
  args: '--gpu-memory-utilization "0.8" --dtype "half" --max-model-len "8200"'

networking:
  port:
    number: 8000

ingress:
  enabled: true
  className: ""
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
  hostname: "chart-example.local"

s3:
  enabled: true # Set to true to use S3
  s3ModelPath: ""
  accessKeyId: ""
  endpoint: ""
  defaultRegion: ""
  secretAccessKey: ""
  sessionToken: ""

nodeSelector: {}
